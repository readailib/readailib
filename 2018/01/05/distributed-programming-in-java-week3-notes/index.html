<!DOCTYPE html>
<html lang="en-us">
  <head>
  	<link rel="shortcut icon" href="http://rh01.github.io/favicon.ico" type="image/x-icon" />
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.31.1" />


<title>分布式编程第三周课堂笔记 - ReadAILib Official Website </title>
<meta property="og:title" content="分布式编程第三周课堂笔记 - ReadAILib Official Website ">
<meta property="author" content="{        map[]} - rh01">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />




<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?7a2217d87addd378e56b4f1e1f6f6019";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?7a2217d87addd378e56b4f1e1f6f6019";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/readailib">GitHub</a></li>
    
    <li><a href="https://www.youtube.com/channel/UCTOuJy4b9E89zCwffX-qC3A">YouTube</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title" style="text-align: center">分布式编程第三周课堂笔记</h1>

    
    
    <div class="post-meta" style="text-align: center">  <a href="http://github.com/rh01" >Heng-Heng Shen</a>  &nbsp;•&nbsp; 2018/01/05 &nbsp;•&nbsp; 12 min read </div>
    

    <div class="article-content">
      <p>本笔记来自课堂记录和课程网站的总结，<strong>转载请注明出处</strong>!</p>
<section id="single-program-multiple-data-spmd-model" class="level2">
<h2>Single Program Multiple Data (SPMD) Model</h2>
<figure>
<img src="http://olrs8j04a.bkt.clouddn.com/18-1-5/83441117.jpg" alt="Figure1: SPMD" /><figcaption>Figure1: SPMD</figcaption>
</figure>
<p><strong>Lecture Summary:</strong> In this lecture, we studied the Single Program Multiple Data (SPMD) model, which can enable the use of a cluster of distributed nodes as a single parallel computer. Each node in such a cluster typically consist of a multicore processor, a local memory, and a network interface card (NIC) that enables it to communicate with other nodes in the cluster. One of the biggest challenges that arises when trying to use the distributed nodes as a single parallel computer is that of data distribution. In general, we would want to allocate large data structures that span multiple nodes in the cluster; this logical view of data structures is often referred to as a global view. However, a typical physical implementation of this global view on a cluster is obtained by distributing pieces of the global data structure across different nodes, so that each node has a local view of the piece of the data structure allocated in its local memory. In many cases in practice, the programmer has to undertake the conceptual burden of mapping back and forth between the logical global view and the physical local views. Since there is one logical program that is executing on the individual pieces of data, this abstraction of a cluster is referred to as the Single Program Multiple Data (SPMD) model.</p>
<p>In this module, we will focus on a commonly used implementation of the SPMD model, that is referred to as the Message Passing Interface (MPI). When using MPI, you designate a fixed set of processes that will participate for the entire lifetime of the global application. It is common for each node to execute one MPI process, but it is also possible to execute more than one MPI process per multicore node so as to improve the utilization of processor cores within the node. Each process starts executing its own copy of the MPI program, and starts by calling the <span class="math inline">\(\mathtt {mpi.MPI\_Init()}\)</span> method, where <span class="math inline">\(\mathtt {mpi}\)</span> is the instance of the MPI class used by the process. After that, each process can call the <span class="math inline">\(\mathtt{mpi.MPI\_Comm\_size(mpi.MPI\_COMM\_WORLD)}\)</span> method to determine the total number of processes participating in the MPI application, and the <span class="math inline">\(\mathtt {MPI\_Comm\_rank(mpi.MPI\_COMM\_WORLD})\)</span> method to determine the process’ own rank within the range, <span class="math inline">\(0…(S−1)\)</span>, where <span class="math inline">\(\mathtt {S=MPI\_Comm\_size()}\)</span>.</p>
<p>In this lecture, we studied how a global view, <span class="math inline">\(XG\)</span>, of array X can be implemented by S local arrays (one per process) of size, <span class="math inline">\(XL.length = XG.length/S\)</span>. For simplicity, assume that XG.length is a multiple of S. Then, if we logically want to set <span class="math inline">\(XG[i]:=i\)</span> for all logical elements of XG, we can instead set XL[i]:=L×R+i in each local array, where L = XL.length, and <span class="math inline">\(\mathtt {R=MPI\_Comm\_rank()}\)</span>. Thus process 0’s copy of XL will contain logical elements <span class="math inline">\(XG[0…L−1]\)</span>, process 1’s copy of XL will contain logical elements <span class="math inline">\(XG[L…2×L−1]\)</span>, and so on. Thus, we see that the SPMD approach is very different from client server programming, where each process can be executing a different program.</p>
</section>
<section id="point-to-point-communication" class="level2">
<h2>Point-to-Point Communication</h2>
<figure>
<img src="http://olrs8j04a.bkt.clouddn.com/18-1-5/39454835.jpg" alt="Figure2: Point-to-Point Communication" /><figcaption>Figure2: Point-to-Point Communication</figcaption>
</figure>
<p><strong>Lecture Summary:</strong> In this lecture, we studied how to perform point-to-point communication in MPI by sending and receiving messages. In particular, we worked out the details for a simple scenario in which process 0 sends a string, “ABCD’’, to process 1. Since MPI programs follow the SPMD model, we have to ensure that the same program behaves differently on processes 0 and 1. This was achieved by using an if-then-else statement that checks the value of the rank of the process that it is executing on. If the rank is zero, we include the necessary code for calling <span class="math inline">\(\mathtt{MPI\_Send()}\)</span>; otherwise, we include the necessary code for calling <span class="math inline">\(\mathtt{MPI\_Recv()}\)</span> (assuming that this simple program is only executed with two processes). Both calls include a number of parameters. The <span class="math inline">\(\mathtt{MPI\_Send()}\)</span> call specifies the substring to be sent as a subarray by providing the string, offset, and data type, as well as the rank of the receiver, and a tag to assist with matching send and receive calls (we used a tag value of 99 in the lecture). The <span class="math inline">\(\mathtt{MPI\_Recv()}\)</span> call (in the else part of the if-then-else statement) includes a buffer in which to receive the message, along with the offset and data type, as well as the rank of the sender and the tag. Each send/receive operation waits (or is blocked) until its dual operation is performed by the other process. Once a pair of parallel and compatible <span class="math inline">\(\mathtt{MPI\_Send()}\)</span> and <span class="math inline">\(\mathtt{MPI\_Recv()}\)</span> calls is matched, the actual communication is performed by the MPI library. This approach to matching pairs of send/receive calls in SPMD programs is referred to as two-sided communication.</p>
<p>As indicated in the lecture, the current implementation of MPI only supports communication of (sub)arrays of primitive data types. However, since we have already learned how to serialize and deserialize objects into/from bytes, the same approach can be used in MPI programs by communicating arrays of bytes.</p>
</section>
<section id="message-ordering-and-deadlock" class="level2">
<h2>Message Ordering and Deadlock</h2>
<figure>
<img src="http://olrs8j04a.bkt.clouddn.com/18-1-5/30986954.jpg" alt="Figure3: Message Ordering and Deadlock" /><figcaption>Figure3: Message Ordering and Deadlock</figcaption>
</figure>
<p><strong>Lecture Summary:</strong> In this lecture, we studied some important properties of the message-passing model with send/receive operations, namely message ordering and deadlock. For message ordering, we discussed a simple example with four MPI processes, <span class="math inline">\(R0,R1,R2,R3\)</span> (with ranks <span class="math inline">\(0…3\)</span> respectively). In our example, process <span class="math inline">\(R1\)</span> sends message <span class="math inline">\(A\)</span> to process <span class="math inline">\(R0\)</span>, and process <span class="math inline">\(R2\)</span> sends message <span class="math inline">\(B\)</span> to process <span class="math inline">\(R3\)</span>. We observed that there was no guarantee that process <span class="math inline">\(R1&#39;s\)</span> send request would complete before process <span class="math inline">\(R2&#39;s\)</span> request, even if process <span class="math inline">\(R1\)</span> initiated its send request before process <span class="math inline">\(R2\)</span>. Thus, there is no guarantee of the temporal ordering of these two messages. In MPI, the only guarantee of message ordering is when multiple messages are sent with the same sender, receiver, data type, and tag – these messages will all be ordered in accordance with when their send operations were initiated.</p>
<p>We learned that send and receive operations can lead to an interesting parallel programming challenge called deadlock. There are many ways to create deadlocks in MPI programs. In the lecture, we studied a simple example in which process <span class="math inline">\(R0\)</span> attempts to send message <span class="math inline">\(X\)</span> to process <span class="math inline">\(R1\)</span>, and process R1 attempts to send message <span class="math inline">\(Y\)</span> to process <span class="math inline">\(R0\)</span>. Since both sends are attempted in parallel, processes R0 and <span class="math inline">\(R1\)</span> remain blocked indefinitely as they wait for matching receive operations, thus resulting in a classical deadlock cycle.</p>
<p>We also learned two ways to fix such a deadlock cycle. The first is by interchanging the two statements in one of the processes (say process <span class="math inline">\(R1\)</span>). As a result, the send operation in process R0 will match the receive operation in process <span class="math inline">\(R1\)</span>, and both processes can move forward with their next communication requests. Another approach is to use MPI’s <span class="math inline">\(\mathtt{sendrecv()}\)</span> operation which includes all the parameters for the send and for the receive operations. By combining send and receive into a single operation, the MPI runtime ensures that deadlock is avoided because a <span class="math inline">\(\mathtt{sendrecv()}\)</span> call in process <span class="math inline">\(R0\)</span> can be matched with a <span class="math inline">\(\mathtt{sendrecv()}\)</span> call in process <span class="math inline">\(R1\)</span> instead of having to match individual send and receive operations.</p>
</section>
<section id="non-blocking-communications" class="level2">
<h2>Non-Blocking Communications</h2>
<figure>
<img src="http://olrs8j04a.bkt.clouddn.com/18-1-5/91505338.jpg" alt="Figure4: Non-Blocking Communications" /><figcaption>Figure4: Non-Blocking Communications</figcaption>
</figure>
<p>Lecture Summary: In this lecture, we studied non-blocking communications, which are implemented via the <span class="math inline">\(\mathtt{MPI\_Isend()}\)</span> and <span class="math inline">\(\mathtt{MPI\_Irecv()}\)</span> API calls. The <span class="math inline">\(I\)</span> in <span class="math inline">\(\mathtt{MPI\_Isend}\)</span> and <span class="math inline">\(\mathtt{MPI\_Irec}v\)</span> stands for “Immediate’’ because these calls return immediately instead of blocking until completion. Each such call returns an object of type <span class="math inline">\(\mathtt{MPI\_Request}\)</span> which can be used as a handle to track the progress of the corresponding send/receive operation. In the example we studied, process <span class="math inline">\(R0\)</span> performs an Isend operation with req0 as the return value. Likewise process <span class="math inline">\(R1\)</span> performs an Irecv operation with req1 as the return value. Further, process <span class="math inline">\(R0\)</span> can perform some local computation in statement <span class="math inline">\(S2\)</span> while waiting for its Isend operation to complete, and process <span class="math inline">\(R1\)</span> can do the same with statement <span class="math inline">\(S5\)</span> while waiting for its Irecv operation to complete. Finally, when process <span class="math inline">\(R1\)</span> needs to use the value received from process <span class="math inline">\(R0\)</span> in statement <span class="math inline">\(S6\)</span>, it has to first perform an <span class="math inline">\(\mathtt{MPI\_Wait()}\)</span> operation on the req1 object to ensure that the receive operation has fully completed. Likewise, when process R0 needs to ensure that the buffer containing the sent message can be overwritten, it has to first perform an <span class="math inline">\(\mathtt{MPI\_Wait()}\)</span> operation on the req0 object to ensure that the send operation has fully completed. For convenience, <span class="math inline">\(\mathtt{MPI\_Waitall()}\)</span> can be invoked on an array of requests to wait on all of them with a single API call.</p>
<p>The main benefit of this approach is that the amount of idle time spent waiting for communications to complete is reduced when using non-blocking communications, since the <span class="math inline">\(\mathtt{Isend}\)</span> and <span class="math inline">\(\mathtt{Irecv}\)</span> operations can be overlapped with local computations. As a convenience, MPI also offers two additional wait operations, <span class="math inline">\(\mathtt{MPI\_Waitall()}\)</span> and <span class="math inline">\(\mathtt{MPI\_Waitany()}\)</span>, that can be used to wait for all and any one of a set of requests to complete. Also, while it is common for <span class="math inline">\(\mathtt{Isend}\)</span> and <span class="math inline">\(\mathtt{Irecv}\)</span> operations to be paired with each other, it is also possible for a nonblocking <span class="math inline">\(\mathtt{send/receive}\)</span> operation in one process to be paired with a blocking <span class="math inline">\(\mathtt{receive/send}\)</span> operation in another process. In fact, a blocking <span class="math inline">\(\mathtt{Send/Recv}\)</span> operation can also be viewed as being equivalent to a nonblocking <span class="math inline">\(\mathtt{Isend/Irecv}\)</span> operation that is immediately followed by a Wait operation.</p>
</section>
<section id="collective-communication" class="level2">
<h2>Collective Communication</h2>
<figure>
<img src="http://olrs8j04a.bkt.clouddn.com/18-1-5/84630111.jpg" alt="Figure5: Collective Communication" /><figcaption>Figure5: Collective Communication</figcaption>
</figure>
<p><strong>Lecture Summary:</strong> In this lecture, we studied collective communication, which can involve multiple processes, in a manner that is both similar to, and more powerful, than multicast and publish-subscribe operations. We first considered a simple broadcast operation in which rank <span class="math inline">\(R0\)</span> needs to send message X to all other MPI processes in the application. One way to accomplish this is for <span class="math inline">\(R0\)</span> to send individual (point-to-point) messages to processes <span class="math inline">\(R1,R2,…\)</span> one by one, but this approach will make <span class="math inline">\(R0\)</span> a sequential bottleneck when there are (say) thousands of processes in the MPI application. Further, the interconnection network for the compute nodes is capable of implementing such broadcast operations more efficiently than point-to-point messages. Collective communications help exploit this efficiency by leveraging the fact that all MPI processes execute the same program in an SPMD model. For a broadcast operation, all MPI processes execute an <span class="math inline">\(\mathtt{MPI\_Bcast()}\)</span> API call with a specified root process that is the source of the data to be broadcasted. A key property of collective operations is that each process must wait until all processes reach the same collective operation, before the operation can be performed. This form of waiting is referred to as a barrier. After the operation is completed, all processes can move past the implicit barrier in the collective call. In the case of <span class="math inline">\(\mathtt{MPI\_Bcast()}\)</span>, each process will have obtained a copy of the value broadcasted by the root process.</p>
<p>MPI supports a wide range of collective operations, which also includes reductions. The reduction example discussed in the lecture was one in which process <span class="math inline">\(R2\)</span> needs to receive the sum of the values of element <span class="math inline">\(Y[0]\)</span> in all the processes, and store it in element <span class="math inline">\(Z[0]\)</span> in process <span class="math inline">\(R2\)</span>. To perform this computation, all processes will need to execute a Reduce() operation (with an implicit barrier). The parameters to this call include the input array (<span class="math inline">\(Y\)</span>), a zero offset for the input value, the output array (<span class="math inline">\(Z\)</span>, which only needs to be allocated in process <span class="math inline">\(R2\)</span>), a zero offset for the output value, the number of array elements (1) involved in the reduction from each process, the data type for the elements (<span class="math inline">\(\mathtt{MPI.INT}\)</span>), the operator for the reduction (<span class="math inline">\(\mathtt{MPI\_SUM}\)</span>), and the root process (<span class="math inline">\(R2\)</span>) which receives the reduced value. Finally, it is interesting to note that MPI’s SPMD model and reduction operations can also be used to implement the MapReduce paradigm. All the local computations in each MPI process can be viewed as map operations, and they can be interspersed with reduce operations as needed.</p>
</section>
<section id="implementation-of-matrix-multiply" class="level2">
<h2>Implementation of Matrix Multiply</h2>
<pre class="java"><code>public static void parallelMatrixMultiply(Matrix a, Matrix b, Matrix c,
            final MPI mpi) throws MPIException {

        final int myrank = mpi.MPI_Comm_rank(mpi.MPI_COMM_WORLD);
        final int size = mpi.MPI_Comm_size(mpi.MPI_COMM_WORLD);

        final int nrows = c.getNRows();
        final int ncols = c.getNCols();
        final int rowChunk = (nrows + size - 1) / size;
        final int startRow = myrank * rowChunk;
        final int endRow = Math.min((myrank + 1) * rowChunk, nrows);

        mpi.MPI_Bcast(a.getValues(), 0, a.getNRows() * a.getNCols(), 0, mpi.MPI_COMM_WORLD);
        mpi.MPI_Bcast(b.getValues(), 0, b.getNRows() * b.getNCols(), 0, mpi.MPI_COMM_WORLD);

        for (int i = startRow; i &lt; endRow; i++) {
            for (int j = 0; j &lt; ncols; j++) {
                c.set(i, j, 0.0);
                for (int k = 0; k &lt; b.getNRows(); k++) {
                    c.incr(i, j, a.get(i, k) * b.get(k, j));
                }
            }
        }

        if (myrank == 0) {
            final MPI.MPI_Request[] requests = new MPI.MPI_Request[size - 1];
            for (int i = 1; i &lt; size; i++) {
                final int rankStartRow = i * rowChunk;
                int rankEndRow = (i +1) * rowChunk;
                // final int rankEndRow = Math.min((i + 1) * rowChunk, nrows);
                if (rankEndRow &gt; nrows) rankEndRow = nrows;

                final int rowOffset = rankStartRow * c.getNCols();
                final int nElements = (rankEndRow - rankStartRow) * c.getNCols();

                requests[i - 1] = mpi.MPI_Irecv(c.getValues(), rowOffset,
                        nElements, i, i, mpi.MPI_COMM_WORLD);
            }
            mpi.MPI_Waitall(requests);
        } else {
            mpi.MPI_Send(c.getValues(), startRow * ncols, (endRow - startRow) * ncols, 0, myrank, mpi.MPI_COMM_WORLD);
        }

    }</code></pre>
</section>
<section id="mpi-installation" class="level2">
<h2>MPI Installation</h2>
<p>If you would like to test your solution on your local machine, you will need to install an MPI implementation.</p>
<p>If you are using Ubuntu, you can install OpenMPI with the following commands:</p>
<pre class="bash"><code>$ sudo apt-get update
$ sudo apt-get install -y openmpi-bin libopenmpi-dev</code></pre>
<p>If you are using Linux or Mac OS, we recommend downloading the OpenMPI implementation from:</p>
<p><a href="https://www.open-mpi.org/software/ompi/v2.0/" class="uri">https://www.open-mpi.org/software/ompi/v2.0/</a></p>
<p>and following the build instructions in the “User Builds” section of the included INSTALL file. Following installation, you must also add the created OpenMPI bin/ folder to your PATH and the created OpenMPI lib/ folder to your LD_LIBRARY_PATH (on Linux) or your DYLD_LIBRARY_PATH (on Mac OS).</p>
<p>If you are on Windows or find the installation instructions for OpenMPI difficult to follow, we recommend using print statements and the Cousera autograder to test your code. The Coursera autograder automatically links in an OpenMPI implementation for you.</p>
</section>
<section id="optional-reading" class="level2">
<h2>Optional Reading:</h2>
<ul>
<li>Wikipedia article on the <a href="https://en.wikipedia.org/wiki/SPMD">SPMD model</a></li>
<li>Documentation on <a href="https://www.open-mpi.org/faq/?category=java#java_docs">Open MPI’s Java interface</a></li>
<li>Wikipedia article on an <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface#Example_program">example MPI program</a> (written in C, not Java).</li>
<li>Documentation on <a href="https://www.open-mpi.org/doc/current/man3/MPI_Send.3.php">MPI Send()</a> and <a href="https://www.open-mpi.org/doc/current/man3/MPI_Recv.3.php">MPI Recv()</a> API’s (in C/C++, not Java)</li>
<li>Wikipedia article on <a href="https://en.wikipedia.org/wiki/Deadlock">Deadlock</a></li>
<li>Documentation on <a href="https://www.open-mpi.org/doc/current/man3/MPI_Isend.3.php">MPI Isend()</a> and <a href="https://www.open-mpi.org/doc/current/man3/MPI_Irecv.3.php">MPI Irecv()</a> API’s (in C/C++, not Java)</li>
<li>Documentation on<a href="https://www.open-mpi.org/doc/current/man3/MPI_Bcast.3.php">MPI Bcast()</a> and <a href="https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php">MPI Reduce()</a> API’s (in C/C++, not Java)</li>
</ul>
</section>

        <hr />

    </div>
    



  </article>

      <footer class="footer">

        

          <div class="generic-box">
Subscribe to our <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>.

  <div class="share-page">
<div class="share-links" style="text-align:center;">
    <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>进行许可。
</div>
</div>


</div>

        
      </footer>

    
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/Dockerfile.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/python.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/scala.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-111305334-1', 'auto');
ga('send', 'pageview');
</script>

  </body>
</html>


  
  

  <div id="gitalk-container"></div>
</div>

<link rel="stylesheet" href="/css/gitalk.css" media="all">
<style type="text/css">
	
.gt-container .gt-comment-admin .gt-header-comment .gt-comment-content {
    background-color: #f5f3d5;
}

textarea {
   background-color: #f5f3d5;
}
</style>
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: 'a98bac7d9ab86f4813de',
        clientSecret: '307c20444af8207286b5f78f7a5ed941786edad0',
        repo: 'readailib',
        owner: 'rh01',
        admin: ['rh01'],
        body: location.href,
      });
      gitalk.render('gitalk-container');
 </script>



</main>


